{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$ \\LaTeX \\text{ command declarations here.}\n",
    "\\newcommand{\\N}{\\mathcal{N}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\norm}[1]{\\|#1\\|_2}\n",
    "\\newcommand{\\d}{\\mathop{}\\!\\mathrm{d}}\n",
    "\\newcommand{\\qed}{\\qquad \\mathbf{Q.E.D.}}\n",
    "\\newcommand{\\vx}{\\mathbf{x}}\n",
    "\\newcommand{\\vy}{\\mathbf{y}}\n",
    "\\newcommand{\\vt}{\\mathbf{t}}\n",
    "\\newcommand{\\vb}{\\mathbf{b}}\n",
    "\\newcommand{\\vw}{\\mathbf{w}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EECS 445:  Machine Learning\n",
    "## Hands On 05:  Linear Regression II\n",
    "* Instructor:  **Zhao Fu, Valli, Jacob Abernethy and Jia Deng**\n",
    "* Date:  September 26, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem 1a: MAP estimation for Linear Regression with unusual Prior\n",
    "Assume we have $n$ vectors $\\vec{x}_1, \\cdots, \\vec{x}_n$. We also assume that for each $\\vec{x}_i$ we have observed a *target* value $t_i$, where\n",
    "$$\n",
    "\\begin{gather}\n",
    "t_i = \\vec{w}^T \\vec{x_i} + \\epsilon \\\\ \n",
    "\\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})\n",
    "\\end{gather}\n",
    "$$\n",
    "where $\\epsilon$ is the \"noise term\".\n",
    "\n",
    "(a) Quick quiz: what is the likelihood *given* $\\vec{w}$? That is, what's $p(t_i | \\vec{x}_i, \\vec{w})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem 1: MAP estimation for Linear Regression with unusual Prior\n",
    "Assume we have $n$ vectors $\\vec{x}_1, \\cdots, \\vec{x}_n$. We also assume that for each $\\vec{x}_i$ we have observed a *target* value $t_i$, sampled IID. We will also put a *prior* on $\\vec{w}$, using PSD matrix $\\Sigma$.\n",
    "$$\n",
    "\\begin{gather}\n",
    "t_i = \\vec{w}^T \\vec{x_i} + \\epsilon \\\\ \n",
    "\\epsilon \\sim \\mathcal{N}(0, \\beta^{-1}) \\\\\n",
    "\\vec{w} \\sim \\mathcal{N}(0, \\Sigma)\n",
    "\\end{gather}\n",
    "$$\n",
    "Note: the difference here is that our prior is a multivariate gaussian with *non-identity* covariance! Also we let $\\mathcal{X} = \\{\\vec{x}_1, \\cdots, \\vec{x}_n\\}$\n",
    "\n",
    "(a) Compute the log posterior function, $\\log p(\\vec{w}|\\vec{t}, \\mathcal{X},\\beta)$\n",
    "\n",
    "*Hint*: use Bayes' Rule\n",
    "\n",
    "(b) Compute the MAP estimate of $\\vec{w}$ for this model\n",
    "\n",
    "*Hint*: the solution is very similar to the MAP estimate for a gaussian prior with identity covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem 2: Handwritten digit classification with logistic regression\n",
    "\n",
    "Download the file `mnist_49_3000.mat` from Canvas. This is a subset of the MNIST handwritten digit database, which is a well-known benchmark database for classification algorithms. This subset contains examples of the digits 4 and 9. The data file contains variables x and y, with the former containing patterns and the latter labels. The images are stored as column vectors.\n",
    "\n",
    "**Exercise**:\n",
    "* Load data and visualize data\n",
    "* Add bias to the feature $\\phi(\\vx)^T = [1, \\vx^T]$\n",
    "* Split dataset into training set with the first 2000 data and test set with the last 1000 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Enable plot here inside Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# all the packages you need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# load data from .mat\n",
    "mat = scipy.io.loadmat('mnist_49_3000.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Implement Newtonâ€™s method to find a minimizer of the regularized negative log likelihood. Try setting $\\lambda$ = 10. Use the first 2000 examples as training data, and the last 1000 as test data. \n",
    "\n",
    "**Exercise**\n",
    "* Implement the loss function with the following formula:\n",
    "$$\n",
    "\\begin{align}\n",
    "E(\\vw) \n",
    "&= -\\ln P(\\vy = \\vt| \\mathcal{X}, \\vw) \\\\\n",
    "&= \\boxed{\\sum \\nolimits_{n=1}^N \\left[ t_n \\ln (1+\\exp(-\\vw^T\\phi(\\vx_n))) + (1-t_n) \\ln(1+\\exp(\\vw^T\\phi(\\vx_n))) \\right] + \\lambda \\vw^T\\vw}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "* Implement the gradient of loss $$\\nabla_\\vw E(\\vw) = \\boxed{ \\Phi^T \\left( \\sigma(\\Phi \\vw) - \\vt \\right) + \\lambda \\vw}$$\n",
    "where $\\sigma(a) = \\frac{\\exp(a)}{1+\\exp(a)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall: Newton's Method\n",
    "$$\n",
    "\\vx_{n+1}= \\vx_n - \\left(\\nabla^2 f(\\vx_n)\\right)^{-1} \\nabla_\\vx f(\\vx_n)\n",
    "$$\n",
    "of which $\\nabla^2 f(\\vx_n)$ is **Hessian matrix** which is the *second order derivative*\n",
    "$$\n",
    "\\nabla^2 f = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1\\partial x_1} & \\cdots & \\frac{\\partial f}{\\partial x_1\\partial x_n}\\\\ \n",
    "\\vdots & \\ddots & \\vdots\\\\ \n",
    "\\frac{\\partial f}{\\partial x_n\\partial x_1} & \\cdots & \\frac{\\partial f}{\\partial x_n\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla^2 E(\\vw) \n",
    "&= \\nabla_\\vw \\nabla_\\vw E(\\vw) \\\\\n",
    "&= \\sum \\nolimits_{n=1}^N \\phi(\\vx_n) r_n(\\vw) \\phi(\\vx_n)^T + \\lambda I\n",
    "\\end{align}\n",
    "$$\n",
    "of which $r_n(\\vw) = \\sigma(\\vw^T \\phi(\\vx_n)) \\cdot ( 1 - \\sigma(\\vw^T \\phi(\\vx_n)) )$\n",
    "\n",
    "**Exercise** \n",
    "* Implement $r_n(\\vw)$\n",
    "* Implement $\\nabla^2 E(\\vw)$\n",
    "* Implement update function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise**\n",
    "* Implement training process(When to stop iterating?)\n",
    "* Implement test function\n",
    "* Compute the test error\n",
    "* Compute the value of the objective function at the optimum"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
